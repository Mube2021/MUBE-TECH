{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNta1u8nmD0/HWcvpN0oyk+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mube2021/MUBE-TECH/blob/main/bart%20for%20afaanoromo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1igTlROWjR0N"
      },
      "outputs": [],
      "source": [
        "import sumy\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "\n",
        "#LSA algorithm\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "#text: text to summarize\n",
        "#no_sentences: number of sentences in your summary,\n",
        "#lang: language of text\n",
        "def lsa_summary(text, no_sentences, lang):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(lang))\n",
        "    lsa_sum = LsaSummarizer()\n",
        "    summary = lsa_sum(parser.document, no_sentences)\n",
        "    gc.collect()\n",
        "    return [str(sentence) for sentence in summary]\n",
        "\n",
        "\n",
        "#Luhn\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "#text: text to summarize\n",
        "#no_sentences: number of sentences in your summary,\n",
        "#lang: language of text\n",
        "def luhn_summary(text, no_sentences, lang):\n",
        "    parser = PlaintextParser(text, Tokenizer(lang))\n",
        "    luhn_sum = LuhnSummarizer()\n",
        "    summary = luhn_sum(parser.document, no_sentences)\n",
        "    gc.collect()\n",
        "    return [str(sentence) for sentence in summary]\n",
        "\n",
        "#LexRank\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "#text: text to summarize\n",
        "#no_sentences: number of sentences in your summary,\n",
        "#lang: language of text\n",
        "def lex_summary(text, no_sentences, lang):\n",
        "    parser = PlaintextParser.from_string(text,Tokenizer(lang))\n",
        "    lex_sum = LexRankSummarizer()\n",
        "    summary = lex_sum(parser.document, no_sentences)\n",
        "    gc.collect()\n",
        "    return[str(sentence) for sentence in summary]\n",
        "\n",
        "#KL\n",
        "from sumy.summarizers.kl import KLSummarizer\n",
        "#text: text to summarize\n",
        "#no_sentences: number of sentences in your summary,\n",
        "#lang: language of text\n",
        "def kl_summary(text, no_sentences, lang):\n",
        "    parser = PlaintextParser.from_string(text,Tokenizer(lang))\n",
        "    kl_summarizer=KLSummarizer()\n",
        "    summary=kl_summarizer(parser.document,sentences_count=no_sentences)\n",
        "    gc.collect()\n",
        "    return [str(sentence) for sentence in summary]\n",
        "\n",
        "#Transformers T5\n",
        "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
        "#text: text to summarize\n",
        "#model: t5-base, t5-small, t5-large, t5-3b, t5-11b\n",
        "def t5_summary(text, model):\n",
        "    my_model = T5ForConditionalGeneration.from_pretrained(model)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model)\n",
        "    input_ids=tokenizer.encode(\"summarize:\"+text, return_tensors='pt', max_length = 512, truncation=True)\n",
        "    summary_ids = my_model.generate(input_ids)\n",
        "    t5_sum = tokenizer.decode(summary_ids[0])\n",
        "    gc.collect()\n",
        "    return(str(t5_sum))\n",
        "\n",
        "#BART\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
        "#text: text to summarize\n",
        "#model: bart-base, bart-large, bart-large-cnn\n",
        "def bart_summary(text, model):\n",
        "    tokenizer=BartTokenizer.from_pretrained('facebook/'+str(model))\n",
        "    model=BartForConditionalGeneration.from_pretrained('facebook/'+str(model))\n",
        "    inputs = tokenizer.batch_encode_plus(text,return_tensors='pt', padding=True, truncation=True)\n",
        "    summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n",
        "    bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    gc.collect()\n",
        "    return(str(bart_summary))"
      ]
    }
  ]
}