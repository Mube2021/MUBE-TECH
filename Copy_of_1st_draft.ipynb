{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp44O0GXU6d31SbbG0e/D6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mube2021/MUBE-TECH/blob/main/Copy_of_1st_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install transformers\n",
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY9wn6pxtbve",
        "outputId": "45dc63f8-e8dd-4432-a82d-30b9b670096f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YIMmk8LQecvx"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.normalizers import NFKC, Sequence\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Customize the tokenizer settings as needed\n",
        "Tokenizer.normalizer = Sequence([NFKC()])\n",
        "Tokenizer.pre_tokenizer = Whitespace()\n",
        "Tokenizer.post_processor = None\n",
        "Tokenizer.decoder = BPE()"
      ],
      "metadata": {
        "id": "5z8xDlWzexRF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data into a Pandas DataFrame\n",
        "df = pd.read_csv(\"datasetv33.csv\")\n"
      ],
      "metadata": {
        "id": "SGSBwaPbeqMN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize a tokenizer with the desired settings\n",
        "tokenizer = Tokenizer(BPE())"
      ],
      "metadata": {
        "id": "9chIF_SUes9O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the tokenizer on the text data\n",
        "trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "tokenizer.train_from_iterator(df[\"headline\"], trainer=trainer)"
      ],
      "metadata": {
        "id": "PP7PXdCle3QJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "3e739aa9-14e8-4090-9b94-6a2df9c30711"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-99c97c168aef>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the tokenizer on the text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBpeTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"[PAD]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[UNK]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[CLS]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[SEP]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[MASK]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headline\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add special tokens to the tokenizer\n",
        "tokenizer.add_special_tokens([\"[BOS]\", \"[EOS]\"])\n"
      ],
      "metadata": {
        "id": "xrqeZfF4e99J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the text data using the trained tokenizer\n",
        "encoded_data = []\n",
        "for text in df[\"text\"]:\n",
        "    # Add the special tokens to the beginning and end of the text\n",
        "    text = f\"[BOS] {text} [EOS]\"\n",
        "    # Tokenize the text\n",
        "    tokens = tokenizer.encode(text)\n",
        "    # Convert the tokens to integer IDs\n",
        "    ids = tokens.ids\n",
        "    encoded_data.append(ids)\n"
      ],
      "metadata": {
        "id": "j3076XKre-Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Pad the integer sequences to the same length\n",
        "padded_data = pad_sequences(encoded_data, padding=\"post\")\n"
      ],
      "metadata": {
        "id": "9zjzW_che-GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = train_test_split(padded_data, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "hjh6Siaee-OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Load the pre-trained BART model and tokenizer\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n"
      ],
      "metadata": {
        "id": "cmHBJkIUe-Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "train_data = torch.tensor(train_data)\n",
        "val_data = torch.tensor(val_data)\n"
      ],
      "metadata": {
        "id": "adX_kCsDfoX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the training parameters\n",
        "num_epochs = 5\n",
        "batch_size = 32\n",
        "learning_rate = 1e-5"
      ],
      "metadata": {
        "id": "0gt1g0s1fysR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# Fine-tune the model on the training data\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(train_data), batch_size):\n",
        "        # Get the batch of training data\n",
        "        batch_data = train_data[i:i+batch_size]\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Generate the summary for the batch using the model\n",
        "        summary_ids = model.generate(batch_data, num_beams=4, max_length=100, early_stopping=True)\n",
        "        # Calculate the loss between the generated summary and the target summary\n",
        "        loss = criterion(summary_ids, batch_data)\n",
        "        # Backpropagate the loss and update the model weights\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "48wV4Qqzf4Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation data after each epoch\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for i in range(0, len(val_data), batch_size):\n",
        "            # Get the batch of validation data\n",
        "            batch_data = val_data[i:i+batch_size]\n",
        "            # Generate the summary for the batch using the model\n",
        "            summary_ids = model.generate(batch_data, num_beams=4, max_length=100, early_stopping=True)\n",
        "            # Calculate the loss between the generated summary and the target summary\n",
        "            loss = criterion(summary_ids, batch_data)\n",
        "            val_loss += loss.item()\n",
        "        val_loss /= len(val_data) // batch_size\n",
        "    print(f\"Epoch {epoch+1}: Val loss = {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "oKZzhOTZgHOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  #this is skipped by gpt but now is here Load the test data\n",
        "test_data = torch.tensor(padded_test_data)\n",
        "\n",
        "# Generate summaries for the test data using the fine-tuned model\n",
        "with torch.no_grad():\n",
        "    test_summaries =\n"
      ],
      "metadata": {
        "id": "a-gccqemgNVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "E7Xp06VFgOxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "DyB2twhYgO1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to generate summary by using fine tuned model use "
      ],
      "metadata": {
        "id": "q12JF4a9gO5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# load the fine-tuned model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained('path/to/fine_tuned_model')\n",
        "\n",
        "# instantiate the summarization pipeline\n",
        "summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# load the test dataset\n",
        "test_dataset = pd.read_csv('path/to/test_data')\n",
        "\n",
        "# create a list to store the summaries\n",
        "summaries = []\n",
        "\n",
        "# iterate through the test dataset and generate a summary for each news article\n",
        "for index, row in test_dataset.iterrows():\n",
        "    article = row['article']\n",
        "    summary = summarizer(article, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
        "    summaries.append(summary)\n",
        "\n",
        "# add the summaries to the test dataset\n",
        "test_dataset['summary'] = summaries\n",
        "\n",
        "# save the test dataset with summaries to a CSV file\n",
        "test_dataset.to_csv('path/to/test_data_with_summaries.csv', index=False)\n"
      ],
      "metadata": {
        "id": "eOgGqOtdgO8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S7jEN_ongO--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To use different models and evaluate them using ROUGE, you can follow these general steps:**"
      ],
      "metadata": {
        "id": "8XAZKjnNjQsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "# Load BERT model\n",
        "bert_model_name = \"bert-base-uncased\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = AutoModelForSeq2SeqLM.from_pretrained(bert_model_name)\n"
      ],
      "metadata": {
        "id": "GtPro2UmjWNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load T5 model\n",
        "t5_model_name = \"t5-base\"\n",
        "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_name)\n",
        "t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_model_name)\n"
      ],
      "metadata": {
        "id": "Nnq8J0vxjhpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "# Define fine-tuning arguments\n",
        "bert_training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy = \"steps\",\n",
        "    eval_steps = 500,\n",
        "    save_steps = 500,\n",
        "    num_train_epochs = 3,\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size = 4,\n",
        "    warmup_steps = 500,\n",
        "    weight_decay = 0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=1000,\n",
        ")\n",
        "\n",
        "# Fine-tune BERT model\n",
        "bert_trainer = Seq2SeqTrainer(\n",
        "    model=bert_model,\n",
        "    args=bert_training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=bert_tokenizer,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "57fo_rHBjkvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t5_training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy = \"steps\",\n",
        "    eval_steps = 500,\n",
        "    save_steps = 500,\n",
        "    num_train_epochs = 3,\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size = 4,\n",
        "    warmup_steps = 500,\n",
        "    weight_decay = 0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=1000,\n",
        ")\n",
        "\n",
        "# Fine-tune T5 model\n",
        "t5_trainer = Seq2SeqTrainer(\n",
        "    model=t5_model,\n",
        "    args=t5_training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=t5_tokenizer,\n",
        ")\n"
      ],
      "metadata": {
        "id": "qc4o4ZMUjk3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GBYYaAxljk6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the models: Evaluate each model on your test dataset using the ROUGE metric. You can use the rouge library in Python to compute the ROUGE scores.**bold text**"
      ],
      "metadata": {
        "id": "yNwbVlXJj98S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "# Initialize the ROUGE metric\n",
        "rouge = Rouge()\n",
        "# Evaluate BERT model\n",
        "bert_predictions = bert_trainer.predict(test_dataset=test_dataset)\n",
        "bert_predictions = bert_tokenizer.batch_decode(bert_predictions.predictions, skip_special_tokens=True)\n",
        "bert_scores = rouge.get_scores(bert_predictions, test_targets)\n",
        "\n"
      ],
      "metadata": {
        "id": "KBxFKxn1jk9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate T5 model\n",
        "t5_predictions = t5_trainer.predict(test_dataset=test_dataset)\n",
        "t5_predictions = t5_tokenizer.batch_decode(t5_predictions.predictions, skip_special_tokens=True)\n",
        "t5_scores = rouge.get_scores(t5_predictions, test_targets)"
      ],
      "metadata": {
        "id": "4H9HaEuvjlAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Print the ROUGE scores for both models\n",
        "print(\"BERT scores:\", bert_scores)\n",
        "print(\"T5 scores:\", t5_scores)\n"
      ],
      "metadata": {
        "id": "KQiDvZ_fjlD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iu_5vMgRjlHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**to show this values in figurative form usng the above code **"
      ],
      "metadata": {
        "id": "dalJirj_lIJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the ROUGE metric\n",
        "rouge = Rouge()\n",
        "\n",
        "# Evaluate BERT model\n",
        "bert_predictions = bert_trainer.predict(test_dataset=test_dataset)\n",
        "bert_predictions = bert_tokenizer.batch_decode(bert_predictions.predictions, skip_special_tokens=True)\n",
        "bert_scores = rouge.get_scores(bert_predictions, test_targets)\n",
        "bert_rouge1_f1 = bert_scores[0]['rouge-1']['f']\n",
        "\n",
        "# Evaluate T5 model\n",
        "t5_predictions = t5_trainer.predict(test_dataset=test_dataset)\n",
        "t5_predictions = t5_tokenizer.batch_decode(t5_predictions.predictions, skip_special_tokens=True)\n",
        "t5_scores = rouge.get_scores(t5_predictions, test_targets)\n",
        "t5_rouge1_f1 = t5_scores[0]['rouge-1']['f']\n",
        "\n",
        "# Plot the ROUGE-1 F1 scores for both models\n",
        "models = ['BERT', 'T5']\n",
        "rouge1_f1_scores = [bert_rouge1_f1, t5_rouge1_f1]\n",
        "\n",
        "plt.bar(models, rouge1_f1_scores)\n",
        "plt.title('ROUGE-1 F1 Scores')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AtEri6xajlLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lvVINOUjlPC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}